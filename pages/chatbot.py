import os
import streamlit as st
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferWindowMemory
from langchain.prompts.chat import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_aws import ChatBedrock
from langchain.callbacks.base import BaseCallbackHandler

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” Default ê°’
if "TEMPERATURE" not in st.session_state:
    st.session_state.TEMPERATURE = 0.9
if "TOP_P" not in st.session_state:
    st.session_state.TOP_P = 1.0
if "TOP_K" not in st.session_state:
    st.session_state.TOP_K = 500
if "MAX_TOKENS" not in st.session_state:
    st.session_state.MAX_TOKENS = 4096
if "MEMORY_WINDOW" not in st.session_state:
    st.session_state.MEMORY_WINDOW = 10


# AWS ë¦¬ì „ ì„¤ì •
os.environ["AWS_DEFAULT_REGION"] = "us-east-1"  

# ëª¨ë¸ ID ì„¤ì •
MODEL_ID = "CLAUDE_3_SONNET_MODEL_ID"

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
CLAUDE_PROMPT = ChatPromptTemplate.from_messages([
    MessagesPlaceholder(variable_name="history"),  # ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•  í”Œë ˆì´ìŠ¤í™€ë”
    HumanMessagePromptTemplate.from_template("{input}"),  # ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì„ í”Œë ˆì´ìŠ¤í™€ë”
])

# ì´ˆê¸° ë©”ì‹œì§€ ì„¤ì •
INIT_MESSAGE = {"role": "assistant",
                "content": "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” Claude ì±—ë´‡ì´ì—ìš”! ë­ë“  ë¬¼ì–´ë´ì£¼ì„¸ìš”! ğŸ˜„"}

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
SYSTEM_PROMPT = "You're a cool assistant, love to response with emoji."

# ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¸ë“¤ëŸ¬ í´ë˜ìŠ¤ ì •ì˜
class StreamHandler(BaseCallbackHandler):
    def __init__(self, container):
        self.container = container
        self.text = ""

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)

# ëŒ€í™” ì²´ì¸ ì´ˆê¸°í™” í•¨ìˆ˜
def init_conversationchain() -> (ConversationChain, ChatBedrock): # type: ignore
    model_kwargs = {'temperature': st.session_state.TEMPERATURE,
                    'top_p': st.session_state.TOP_P,
                    'top_k': st.session_state.TOP_K,
                    'max_tokens': st.session_state.MAX_TOKENS,
                    'system': SYSTEM_PROMPT}  # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì •

    llm = ChatBedrock(
        model_id=MODEL_ID,
        model_kwargs=model_kwargs,
        streaming=True  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
    )

    conversation = ConversationChain(
        llm=llm,
        verbose=True,
        memory=ConversationBufferWindowMemory(
            k=st.session_state.MEMORY_WINDOW, ai_prefix="Assistant", chat_memory=StreamlitChatMessageHistory(), return_messages=True),  # ë©”ëª¨ë¦¬ ì„¤ì •
        prompt=CLAUDE_PROMPT
    )

    # ì„¸ì…˜ ìƒíƒœì— ì´ˆê¸° ë©”ì‹œì§€ ì €ì¥
    if "messages" not in st.session_state.keys():
        st.session_state.messages = [INIT_MESSAGE]

    return conversation, llm

# ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_response(conversation: ConversationChain, input_text: str) -> str:
    return conversation.run(input=input_text, callbacks=[StreamHandler(st.empty())])

# ìƒˆë¡œìš´ ëŒ€í™” ì‹œì‘ í•¨ìˆ˜
def new_chat() -> None:
    st.session_state["messages"] = [INIT_MESSAGE]
    st.session_state["langchain_messages"] = []

# ëŒ€í™” ì²´ì¸ ì´ˆê¸°í™”
conv_chain, llm = init_conversationchain()


# Streamlit 
if __name__ == "__main__":


    st.set_page_config(page_title='ğŸ§‘ğŸ»â€ğŸ’» AI ì±—ë´‡', layout='wide')
    st.title("ğŸ§‘ğŸ»â€ğŸ’» AI ì±—ë´‡")

    # ëŒ€í™” ì²´ì¸ ì´ˆê¸°í™”, llm ê°ì²´ë¥¼ session_stateì— ì €ì¥
    conv_chain, llm = init_conversationchain()
    st.session_state.llm = llm    

    # ì‚¬ì´ë“œë°”ì— íŒŒë¼ë¯¸í„° ì„¤ì • ì¶”ê°€
    with st.sidebar:
        st.markdown("## ì¶”ë¡  íŒŒë¼ë¯¸í„°")
        TEMPERATURE = st.slider("Temperature", min_value=0.0, max_value=1.0, value=1.0, step=0.1)
        TOP_P = st.slider("Top-P", min_value=0.0, max_value=1.0, value=1.00, step=0.01)
        TOP_K = st.slider("Top-K", min_value=1, max_value=500, value=500, step=5)
        MAX_TOKENS = st.slider("Max Token", min_value=0, max_value=4096, value=4096, step=8)
        MEMORY_WINDOW = st.slider("Memory Window", min_value=0, max_value=10, value=10, step=1)

    # ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¸ë“¤ëŸ¬ í´ë˜ìŠ¤ ì •ì˜
    class StreamHandler(BaseCallbackHandler):
        def __init__(self, container):
            self.container = container
            self.text = ""

        def on_llm_new_token(self, token: str, **kwargs) -> None:
            self.text += token
            self.container.markdown(self.text)

    # ì‚¬ì´ë“œë°”ì— ìƒˆë¡œìš´ ëŒ€í™” ì‹œì‘ ë²„íŠ¼ ì¶”ê°€
    st.sidebar.button("New Chat", on_click=new_chat, type='primary')

    # ê¸°ì¡´ ëŒ€í™” ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    prompt = st.chat_input()

    # ì‚¬ìš©ì ì…ë ¥ì´ ìˆì„ ê²½ìš° ì²˜ë¦¬
    if prompt:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ ì–´ì‹œìŠ¤í„´íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° ìƒˆë¡œìš´ ì‘ë‹µ ìƒì„±
    if st.session_state.messages[-1]["role"] != "assistant":
        with st.chat_message("assistant"):
            response = generate_response(conv_chain, prompt)
        message = {"role": "assistant", "content": response}
        st.session_state.messages.append(message)

